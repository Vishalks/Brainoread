<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Brainoread by nishantborude</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">Brainoread</h1>
        <p class="header">This source code of a tool which is a standalone binary classifier using machine learning algorithms from R and Java. Currently used for diagnosis of patients suffering from brain disorders.</p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/nishantborude/Brainoread/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/nishantborude/Brainoread/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/nishantborude/Brainoread">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by <a class="header name" href="https://github.com/nishantborude">nishantborude</a></p>


      </header>
      <section>
        <h1>
<a id="about" class="anchor" href="#about" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>About</h1>

<p>We have created a generic binary classiﬁer which we currently use for diagnosis of patients suﬀering from brain disorders. However, the tool is powerful enough to work on any binary class classiﬁcation problem. On prediction of 119,748 test subjects for Schizophrenia, we got a maximum accuracy of <strong>91.78%</strong>. And on cross validation of Dementia dataset, we got the maximum accuracy of close to <strong>100%</strong>. 
This tool can be easily ported to hospitals and can be used for prediction of any data provided the model is initially trained using similar known dataset. Initially the tool takes time for training all models and then tunes those using diﬀerent tuning parameters, and once the model is trained; prediction can be done in very less time. It provides the advantage of saving the model so it can be reused any number of times. 
Moreover, the prediction accuracy of the model can be increased by manually adding training subjects to the dataset.</p>

<h1>
<a id="features-extracted-from-mri-scans" class="anchor" href="#features-extracted-from-mri-scans" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Features extracted from MRI scans</h1>

<h2>
<a id="fnc-features" class="anchor" href="#fnc-features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>FNC Features</h2>

<p>Functional Network Connectivity (FNC) are relationship values that compress the general association between free cerebrum maps after some time.Therefore, the FNC feature gives a picture of the availability design after sometime between autonomous systems (or brain maps). These FNC features were obtained by Group Independent Component Analysis (GICA) of MRI images. The MRI chosen for the task were of both, healthy people and people suﬀering from Schizophrenia. An arrangement of cerebrum maps and timecourses relating to these mind maps were achieved by the GICA disintegration of the fMRI information. These timecourses demonstrated the action level of the comparing cerebrum map at every point in time. The FNC features are the relationships between these timecourses. For interpretation, FNC demonstrates a subject’s general level of ”synchronicity” between mind areas.As this information is derived from functional MRI scans, FNCs are considered a functional modality feature. They describe patterns of the brain function.</p>

<p><img src="https://cloud.githubusercontent.com/assets/10834446/19800028/6355c83c-9d17-11e6-82b7-cf34fbda4eba.png" alt=""></p>

<h2>
<a id="sbm-features" class="anchor" href="#sbm-features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SBM Features</h2>

<p>Source-Based Morphometry (SBM) loadings relate to the weights of brain maps obtained by applying Independent Component Analysis (ICA) on the gray-matter concentration maps.Gray-matter is the outer layer of the brain. Most of the signal processing of the brain occurs in gray-matter region.For interpretation, gray-matter concentrationindictes the ”computational power” available in a certain region of the brain.ICA investigation of basic MRI for gray-matter focus gives autonomous mind maps whose expres sion levels ﬂuctuate as indicated by the subjects. Basically, a close to zero stacking for a given ICA-determined mind map demonstrates that the cerebrum locales delineated in that guide are modest present in the subject (i.e., the presence of gray-matter concentration areas in those regions are very low in that subject). As this information is derived from structural MRI scans, SBM loadings are considered a structural modality feature. They describe patterns of the brain structure.</p>

<p><img src="https://cloud.githubusercontent.com/assets/10834446/19799291/8abb8aa0-9d13-11e6-839e-93dcbd1365a7.png" alt=""></p>

<h1>
<a id="uses-state-of-the-art-machine-learning-techniques" class="anchor" href="#uses-state-of-the-art-machine-learning-techniques" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Uses State of the Art Machine Learning Techniques</h1>

<p>Most of the algorithms are used individually in the first iterations. The results generated from top models can be sorted according to prediction accuracy and then they can be combined further for increasing accuracy. </p>

<h2>
<a id="k-fold-cross-validation" class="anchor" href="#k-fold-cross-validation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>k-fold Cross Validation</h2>

<p>Cross Validation is the method where we divide the entire training data into parts. k is the parameter through which we decide how many such parts should be made. Of them, we use 1 part for testing and k – 1 parts for training. Such training – testing cycle is repeated until all parts are dropped for testing i.e. the loop runs k times. For a value of k = 10, we divide the dataset into 10 parts; train for 9 parts and test for 1 part. Such process is repeated until every single part is used for testing and remaining 9 for training. Thus, the loop iterates 10 times. </p>

<h2>
<a id="stratified-sampling" class="anchor" href="#stratified-sampling" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Stratified Sampling</h2>

<p>For determining the best model we initially do stratified sampling of the training data. It is similar to k-fold cross validation. Here, we divide the entire data set into ratio of 70:30. We use the createDataPartition function of Caret package and pass parameter as 0.7. The training dataset is divided into random 70% and 30% of which we use 70% for training and 30% for testing. We repeat this procedure of randomly dividing the dataset into 70% and 30% for 100 iterations. The accuracy for each iteration is recorded and finally, average accuracy of prediction of data is calculated by taking the average of 100 accuracies. </p>

<h2>
<a id="feature-selection" class="anchor" href="#feature-selection" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feature Selection</h2>

<p>Feature Selection includes selection of most important features from a list of all features. Feature Selection helps the model to solve the problem of overfitting the training set. That is, it reduces variance from the train set. The methods used for feature selection are: </p>

<ul>
<li>Recursive Feature Elimination (RFE) </li>
<li>Simulated Annealing Feature Selection (SAFS)</li>
<li>varImp for Random Forest </li>
</ul>

<h3>
<a id="fnc-feature-selection" class="anchor" href="#fnc-feature-selection" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>FNC Feature Selection</h3>

<p><img src="https://cloud.githubusercontent.com/assets/10834446/19798727/79ebf960-9d10-11e6-8dc0-eac3f261e8d8.png" alt=""></p>

<h3>
<a id="sbm-feature-selection" class="anchor" href="#sbm-feature-selection" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SBM Feature Selection</h3>

<p><img src="https://cloud.githubusercontent.com/assets/10834446/19798729/7a2bf1b4-9d10-11e6-8aeb-3689df07ad21.png" alt=""></p>

<h1>
<a id="list-of-algorithms" class="anchor" href="#list-of-algorithms" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>List of Algorithms</h1>

<p>Our package uses a list of all classification algorithms available in the mentioned packages. A few of the algorithms used for the project from Caret package are as follows:</p>

<ul>
<li>gpls (Generic Partial Least Squares)</li>
<li>svm (Support Vector Machines)</li>
<li>nb (Naive Bayes)</li>
<li>nnet (Neural Network)</li>
<li>lda (Linear Discriminant Analysis)</li>
<li>rf (Random Forest)</li>
<li>wsrf (Weighted Subspace Random Forest)</li>
<li>LMT (Logistic Model Trees)</li>
<li>glm (Generalized Linear Model)</li>
<li>gbm (Gradient Boosting Machines)</li>
</ul>

<h1>
<a id="system-design" class="anchor" href="#system-design" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>System Design</h1>

<p>The following is a flow chart that explains the work flow of the model.</p>

<p><img src="https://cloud.githubusercontent.com/assets/10834446/19798733/7a766ea6-9d10-11e6-8338-e65616915420.jpg" alt=""></p>

<h1>
<a id="results-and-analysis" class="anchor" href="#results-and-analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results and Analysis</h1>

<h2>
<a id="plot-for-analysis-of-dementia-patients" class="anchor" href="#plot-for-analysis-of-dementia-patients" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Plot for Analysis of Dementia Patients</h2>

<p>Below figure depicts the classification of Dementia patients plot against two features namely AWF and nWBV. We used a visualization function to give a graphical output of the data. The plot in red is people not suffering from Dementia whereas the blue plot shows the people suffering from the disease.</p>

<p><img src="https://cloud.githubusercontent.com/assets/10834446/19800029/6359ef2a-9d17-11e6-82a5-fdf744e89a57.png" alt=""></p>

<h1>
<a id="visible-methods" class="anchor" href="#visible-methods" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visible methods</h1>

<h3>
<a id="1-getbestmodelx-y-iter--100-verboseiter--false" class="anchor" href="#1-getbestmodelx-y-iter--100-verboseiter--false" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1) getBestModel(x, y, iter = 100, verboseIter = FALSE){…}</h3>

<h4>
<a id="file--getbestmodelr" class="anchor" href="#file--getbestmodelr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file : getBestModel.r</h4>

<p>This function takes features(x) and labes(y) of training dataset as input, performs data
preprocessing (replace NA and NaN with 0). Evaluate train data for each model with
stratified sampling (70% - 30%) “iter” times, which is 100 by default. Function
returns a dataframe with model-name, environment, averaged AUC, specificity,
sensitivity, PPV, NPV. This function calls hidden methods getModelList() and
getAvgAUC() to get dataframe to be return with default 0 values and get averaged
values for “iter” iteration for each model in dataframe.</p>

<h3>
<a id="2-gettrainmodelx-y-methodname-verboseiter--false-tunelength--10" class="anchor" href="#2-gettrainmodelx-y-methodname-verboseiter--false-tunelength--10" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2) getTrainModel(x, y, methodName, verboseIter = FALSE, tuneLength = 10){…}</h3>

<h4>
<a id="file--trainpredictr" class="anchor" href="#file--trainpredictr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file : trainPredict.r</h4>

<p>This function takes same features(x), and labels(y) of training dataset as input with
extra argument methodName – this is a row from dataframe returned by
getBestModel(). This function train model if the environment is R then tuneLength
parameter is used. For R environment trainControl() and train() from caret are used,
and if environment is JAVA then train dataset (features and labels) is written in file
and calls the hidden method javaCall(), this function calls java methods using rJava
library. Final model is written in file (currentModel.rds for R and currentModel.obj
for java). This function returns vector with model-name and environment.</p>

<h3>
<a id="3-getpredictprobmodel-testdata" class="anchor" href="#3-getpredictprobmodel-testdata" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3) getPredictProb(model, testData){…}</h3>

<h4>
<a id="file--trainpredictr-1" class="anchor" href="#file--trainpredictr-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file : trainPredict.r</h4>

<p>This function uses model (vector return from getTrainModel) and test dataset as
input. If environment is R model written in currentModel.rds is read and predict()
method from caret is used to get prediction probabilities, and for JAVA environment
test dataset is written in file with extra result file and call is made to javaCall()
method, to get prediction probabilities java class writes result in result_prob.csv file
which is read by hidden method getPredFromFile(). Function returns dataframe with
both class probabilities.</p>

<h3>
<a id="4-getlastmodel" class="anchor" href="#4-getlastmodel" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4) getLastModel(){…}</h3>

<h4>
<a id="file--trainpredictr-2" class="anchor" href="#file--trainpredictr-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file : trainPredict.r</h4>

<p>Method getTrainModel() creates one additional file info.csv which contains labels of
class, model-name and environment information. This function reads that file and
returns a vector same as getTrainModel() returns.</p>

<h1>
<a id="hidden-methods" class="anchor" href="#hidden-methods" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hidden Methods</h1>

<h3>
<a id="1-getmodellist" class="anchor" href="#1-getmodellist" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1) getModelList(){…}</h3>

<h4>
<a id="file--getaucr" class="anchor" href="#file--getaucr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file : getAUC.r</h4>

<p>This function creates a dataframe of model-name, environment and accuracy value fields
with default 0 and returns that dataframe.</p>

<h3>
<a id="2-getavgaucx-y-methodname-iter-verboseiter" class="anchor" href="#2-getavgaucx-y-methodname-iter-verboseiter" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2) getAvgAUC(x, y, methodName, iter, verboseIter){…}</h3>

<h4>
<a id="file--getaucr-1" class="anchor" href="#file--getaucr-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file : getAUC.r</h4>

<p>This method takes features(x), labels(y), method-name (row of model-list) and number of
iterations to be performed. Function returns vector with averaged values of accuracy
values. This function perform stratified sampling (70% - 30%) 70% is used for training
and 30% is used for testing. If environment is R then train() and predict() from caret is
used. If environment is JAVA then train and test dataset is written in files with addition
result file, and a call is made to javaCall() method. Java writes result in file which is read
by getPredFromFile() method. AUC is caluculated using caTools::colAUC() method and
rest values are calculated using caret::confusionMatrix() method.</p>

<h3>
<a id="3-removenanx" class="anchor" href="#3-removenanx" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3) removeNaN(x){…}</h3>

<h4>
<a id="file--getaucr-2" class="anchor" href="#file--getaucr-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file : getAUC.r</h4>

<p>This file replaces NaN values with 0 from dataframe and returns that dataframe .</p>

<h3>
<a id="4-getpredfromfile" class="anchor" href="#4-getpredfromfile" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4) getPredFromFile(){…}</h3>

<h4>
<a id="file--getaucr-3" class="anchor" href="#file--getaucr-3" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file : getAUC.r</h4>

<p>Java writes predictions of positive class in result_prob.csv. This functions reads this file
and place probabilities of negative class and returns dataframe with both class
probabilities.</p>

<h3>
<a id="5-javacall-modelname-train--false-test--false" class="anchor" href="#5-javacall-modelname-train--false-test--false" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5) javaCall (modelName, train = FALSE, test = FALSE){…}</h3>

<h4>
<a id="file-javacallr" class="anchor" href="#file-javacallr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>file :javaCall.R</h4>

<p>This function uses rJava to create object of java class file and calls method of that class
instance. This method uses model-name, with train and test arguments. If train and test
both are false then java methods performs both training and testing (used for stratified
sampling). If train is true then only train model and save in model in file, and if test is
true then read model saved in file test the dataset and write prediction in result_prob.csv</p>

<p><em>Demo script is written in test.r file with dementia dataset (demen.csv)</em></p>
      </section>
      <footer>
        <p><small>Hosted on <a href="https://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		
  </body>
</html>
